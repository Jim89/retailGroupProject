---
title: "Retail Analytics Group Report"
author: Philipp Dufter, Jessica Zhou, Joachim Ernst, Julian Hohlweg, Maria Engesaeth,
  Jim Leach, Tina Buturishvili
date: "17 March 2016"
output:
  pdf_document:
    fig_caption: yes
  word_document: default
---

```{r source, echo = FALSE, message = FALSE, warning = FALSE}
source("./r/00_analysis.R")
library(knitr)
library(dplyr)

# Set up custom theme to be applied to all plot objects
theme <- theme(legend.position = "bottom",
               axis.text.y = element_text(size = 16, colour = "black"),
               axis.text.x = element_text(size = 16, colour = "black"),
               legend.text = element_text(size = 16),
               legend.title = element_text(size = 16),
               title = element_text(size = 16),
               strip.text = element_text(size = 16, colour = "black"),
               strip.background = element_rect(fill = "white"),
               panel.grid.minor.x = element_blank(),
               panel.grid.major.x = element_line(colour = "grey", linetype = "dotted"),
               panel.grid.minor.y = element_line(colour = "lightgrey", linetype = "dotted"),
               panel.grid.major.y = element_line(colour = "grey", linetype = "dotted"),
               panel.margin.y = unit(0.1, units = "in"),
               panel.background = element_rect(fill = "white", colour = "lightgrey"),
               panel.border = element_rect(colour = "black", fill = NA))
```

# Introduction

We were provided with data from _Katar Worldpanel_ covering sales of coffee over a year at several major UK retailers. We were asked to use the data to develop a response to the question: 

> "Do heavy versus light users in the category have differences in price elasticities and differences in buying behaviour (e.g. buy different brands, buy different proportion of products on discount)?

This report presents our response to this question. Whilst the data are confidential, our approach including both intermediate and final analytical code/results is packaged with this report, and can also be found on our online repository here: [https://github.com/Jim89/retailGroupProject](https://github.com/Jim89/retailGroupProject).

## Executive summary

We have found that differences exist between heavy and light users in the category. We measured price elasticity using regression methods and found heavy users to be typically more sensitive to price than light users. We used machine learning approaches to understand differences in buying behaviours between the two groups and were able to successfully show that there are differences in a number of behaviours. 

***

# Section One: Approach

## Filtering and cleaning the data

In line with the assignment instructions we filtered the data to include only data from the following retailers: Tesco, Asda, Sainsburys, Morrisons, Aldi, and Lidl. We selected only records that covered the following types of coffee: granules, freeze dried, and micro ground.

Using information on promotions we defined a field indicating if the sale was a price-promoted sale (TPR). We derived a second, similar field which indicated if the sale was a unit-promoted sale (e.g. X product for £Y). We calculated the price of each product using the approach outlined in the data-dictionary ($price = netspend/packs$).

We grouped and cleaned the brand names. Any brands with fewer than five thousand sales were labelled as "_Other brand_". All supermarket own-brands were labelled as "_supermarket own_", and all _Nescafe_ products were grouped together. This left us with six distinct brands:

* Carte Noire;
* Kenco;
* Douwe Egberts;
* Nescafe;
* Supermarket own; and
* Other.

Finally, we created a "visit ID" for each unique combination of week, day, house and shop name.

## Defining heavy and light

In order to separate "heavy" from "light" users in the category we calculated for each household the average _weight volume_ of coffee purchased on each shopping visit. Houses below the twenty-fifth percentile were classified as "light" users. Houses above the seventy-fifth percentile were classified as "heavy" users. Houses that fell between the twenty-fifth and seventy-fifth percentile were classified as "medium" users and were excluded from the analysis. 

We note that this method does not account for consumers with combinations of extreme values in the data (for example a customer who appears in the data only once and purchased a very large volume of coffee, compared with a customer who bought a smaller volume every single day). Our exploratory data analyses suggested that such extreme values were virtually non-existent in the data, however. Further, as the data were collected via consumers scanning their receipts, we have made an assumption that each visit for a consumer present in the data is typical of their usual buying behaviour (imagining that not every consumer will scan every receipt for every visit). As such, we believe that our approach for classifying customers in this way is appropriate.

## Data validation and preparation

We also performed validation checks on the data for completeness, looking for:

* missing data values;
* unexpected 0 values;
* duplicate rows; and
* quasi-duplicate rows.

We found no missing values in the data and any 0 values were determined to be legitimate after more investigation. We found duplicate rows in the data. We assumed that duplicates refer to purchases of several packs in scope on the same shop visit.

We identified potentially erroneous `NETSPEND` values. We found observations for which all columns except `NETSPEND` were identical. This was partly explained by the type of promotion (e.g. 3F2). However, some values could not be explained. However, any such errors were economically insignificant. Given the total size of the data, these observations do not meaningfully influence the analysis. As such, we took no action to adjust or correct these values.

We aggregated the data to a weekly level in order to prepare them for elasticity calculations. For each of the brands defined previously, we calculated:

* the total number of sales;
* the average price paid;
* the proportion of total sales that were made on a price-based promotion; and
* the proportion of total sales that were made on a units-based promotion.

We investigated the use of a brand's market share in each week and the use of total sales across all brands in each week. These were not found to be useful and so were removed from the final model. 

## Price elasticity functional form selection

We developed models for each functional form (level-level, semi-log and log-log) for each brand and each customer type (i.e. one model per brand, customer type and functional form). We used five-fold cross validation to estimate the out-of-sample error (using mean-squared error) and averaged this error across the six brands to give a single error estimate for each functional form per customer type. It was found that the best functional form for heavy users was semi-log, whereas it was the level-level form for the light users.

## Price elasticity model implementation

For each of customer type, we implemented step-wise regression to find the best model for each brand. We obtained the relevant coefficients from each brand's model and performed the appropriate transformations to generate elasticity values. 

We transformed the values into elasticity matrices for light and heavy users. We also obtained the $p$-value for each coefficient, setting to zero any elasticities that were not statistically significant ($p \geq 0.05$).

## Buying behaviours

In order to assess differences between light and heavy users we defined a range of buying behaviours. For each house we calculated the:

* average spend (£) per visit;
* average packs purchased per visit;
* average weekly visits;
* shop loyalty;
* brand loyalty;
* maximum amount spent on any single trip;
* minimum amount spent on any single trip;
* proportion of all purchases on price promotion; and
* proportion of all purchases on unit-based promotion.

These measures were combined in to a single data set keyed by house.

## Buying behaviour differences

We applied $k$-means clustering to split the data in to two clusters. We then compared how the clusters found by the algorithm matched with our pre-defined label of "heavy" or "light" for each user. 

Secondly, treating the problem as a binary classification, we applied a random forest algorithm to determine if the data could be used to classify users as light or heavy. Having trained the model, we were able to extract importance measures from the model for each variable. This determined which buying behaviours had the greatest impact in classifying users as light or heavy, i.e. which buying behaviours were the most different between the two groups.

Finally, we performed statistical analyses of differences in buying behaviours between the two user types. We used $t$-tests to assess differences in mean values, $F$-statistics to assess differences in variances, and Mann-Whitney tests for two-sample Wilcoxon tests.

\pagebreak

# Section Two: Results and discussion

## Price elasticity matrices

```{r heavy_elast, echo = FALSE}
colnames(heavy_elasticities_clean) <- colnames(heavy_elasticities_clean) %>% toproper()
rownames(heavy_elasticities_clean) <- rownames(heavy_elasticities_clean) %>% toproper()
heavy_elasticities_clean %>% 
  kable(caption = "Price elasticity matrix for heavy users")
```

```{r light_elast, echo = FALSE}
colnames(light_elasticities_clean) <- colnames(light_elasticities_clean) %>% toproper()
rownames(light_elasticities_clean) <- rownames(light_elasticities_clean) %>% toproper()
light_elasticities_clean %>% 
  kable(caption = "Price elasticity matrix for light users")
```

Heavy users have an average own-price elasticity of `r round(heavy_elasticities_clean %>% diag() %>% mean(), 3)` compared with `r round(light_elasticities_clean %>% diag() %>% mean(), 3)` for light users (tables one and two). Moreover there are more 0 (i.e. insignificant) elasticities for light users (`r sum(light_elasticities_clean ==0)`) than for heavy (`r sum(heavy_elasticities_clean == 0)`). This shows that heavy users are more price sensitive than light users (tables one and two). 

Whilst the differences in own-price elasticity are not statistically significant ($p$-value `r round(t.test(heavy_elasticities_clean %>% diag(), light_elasticities_clean %>% diag())$p.value, 3)`), from a practical perspective the results may still be of interest to a manager in the retail setting. 

We note the presence of some negative cross-price elasticities. These are unusual so we examined the co-occurrence matrices (appendix one) for products with negative cross-price elasticities in order to investigate if they were potentially complementary products. It appears that they are not, and such values are therefore likely to have been produced as a result of aggregating the data to a weekly level. When calculating clout and vulnerability, we have ignored these erroneous values.

## Clout and vulnerability statistics

```{r clout_map, echo = FALSE, fig.cap="Clout and vulnerabilty maps for heavy and light users. Note that negative cross-price elasticities were not included in these calculations as they are believed to erroneous values introduced via aggregation.", fig.height=4, fig.width=6}
source("./r/files/801_clout_and_vuln_map.R")
clout_and_vuln_map
```

There are a few key differences in the clout and vulnerability between the two groups. Firstly supermarket-branded coffees have a large clout for heavy users compared to light. We concluded that as it gets more expensive, heavy purchasers of supermarket coffee are more likely to switch to premium brands at comparable price points. The table of full numerical statistics can be seen in appendix five.

Secondly, Kenco coffee has a high vulnerability for heavy users relative to light users. This drop in vulnerability is interesting, indicating for that heavy users, Kenco is at a greater risk of losing market share to its competitors than for light users. 

Thirdly, Nescafe has a reasonably high clout _and_ vulnerability for heavy users, and very small values for light users.  Nescafe is the most popular brand for heavy users (see appendix 3) and so it makes sense that it has both a high clout and vulnerability given heavy users price sensitivity. However light users appear to prefer supermarket own-brand goods (appendix 3) and are less price sensitive; so Nescafe is neither greatly affected by, or has much effect on, other brands. 

## Buying behaviour differences

The results from both our $k$-means and random forest implementations showed that buying behaviour can be used to distinguish between heavy and light users. This means that there _are_ differences between the two groups. Appendix two shows the confusion matrices found via each of these algorithms. $k$-means was able to distinguish between heavy and light users with `r round(sum(diag(conf_mat))/sum(conf_mat)*100, 1)`% accuracy compared to the labels we defined. Random forest had an accuracy of `r round((1-mean(rf$err.rate[,1]))*100, 1)`% compared to the true labels.

The variable importance plot from the random forest algorithm is seen in figure two. It shows that features such as the average number of packs purchased per visit and the proportion of all purchases made on price promotion are the most important when determining light and heavy users, meaning that they have the greatest differences between the two groups.

```{r var_imp, echo = FALSE, fig.cap="Variable importance (measured by mean decrease in accuracy) for classifying users in to light versus heavy via random forest algorithm.", fig.height=3.5, fig.width=6}
source("./r/files/802_variable_importance_plot.R")
var_imp(rf)
```

Such differences can be readily observed in figures three and four. Figure three shows that heavy users bought a higher proportion of products on price promotion relative to light users, where the majority of users made no price-promotional purchases. Figure four shows that light users typically only purchase an average of one pack per visit, whereas heavy users buy a greater number of packs at once. Further plots can be seen in the packaged code base or in the online repository  here: [https://github.com/Jim89/retailGroupProject](https://github.com/Jim89/retailGroupProject).

```{r bb_promo_price, echo = FALSE, fig.cap = "Differences in proportion of price-promoted sales for light and heavy users", fig.height = 3.5, fig.width=6, warning=FALSE}
source("./r/files/803_buying_behaviour_charts.R")
bb_prop_promo_price
```

```{r bb_max_spend, echo = FALSE, fig.cap = "Differences in maximum single-trip spend for light and heavy users", fig.height=3.5, fig.width=6, warning=FALSE}
bb_avg_weekly_packs
```

We also computed the statistical significance of differences between heavy and light users for a number of buying behaviours. The results each time indicated that the differences were statistically significant (appendix four). 

***

# Appendices

## Appendix one - Co-occurence matrices

```{r cooc_heavy, echo = FALSE}
cooccurence_heavy %>% 
  kable(caption = "Brand co-occurence matrix for heavy users")
```

```{r cooc_light, echo = FALSE}
cooccurence_light %>% 
  kable(caption = "Brand co-occurence matrix for light users")
```

\pagebreak

## Appendix two - $k$-means and random forest confusion matrices

```{r k_conf_mat, echo = FALSE}
rownames(conf_mat) <- c("Cluster one", "Cluster two")
conf_mat %>% 
  kable(caption = "Confusion matrix for k-Means clusters against existing user labels")
```

```{r rf_conf_mat, echo = FALSE}
mat <- rf$confusion
colnames(mat) <- c("Heavy", "Light", "Error")
mat %>%   
  kable(caption = "Confusion matrix for random forest classification against existing user labels")
```

## Appendix three - Brand proportion of purchases differences

```{r prob_brands, echo = FALSE, fig.cap = "", fig.height = 3.5, fig.width=6, warning=FALSE}
source("./r/files/804_brands_purchased_plot.R")
brands_purchase_plot
```

## Appendix four - Brand behaviour statistical differences

```{r man_whit_tests, echo = FALSE}
wilcox_tests %>%
  select(variables, mann.whitney.u.statistic, p.value) %>% 
  mutate(p.value = signif(p.value, 3)) %>% 
  kable(caption = "Mann Whitney U-statistics for of a range of buying behaviours between heavy and light users. Note the p-values of 0 are simply rounded values, indicating that p was well below 0.01.",
        col.names = c("Behaviour", "U-Statistic", "p-value"))
```

```{r var_tests, echo = FALSE}
var_tests %>%
  select(variables, var_heavy, var_light, f.statistic, p.value) %>% 
  mutate(p.value = signif(p.value, 3)) %>% 
  kable(caption = "Variance differences for of a range of buying behaviours between heavy and light users. Note the p-values of 0 are simply rounded values, indicating that p was well below 0.01.",
        col.names = c("Behaviour", "Heavy Users", "Light Users", "F-Statistic", "p-value"))
```

```{r ttests, echo = FALSE}
t_tests %>%
  select(variables, mean_heavy, mean_light, difference, statistic, p.value) %>% 
  mutate(p.value = signif(p.value, 3)) %>% 
  kable(caption = "Mean value differences for of a range of buying behaviours between heavy and light users. We conducted these tests after viewing the results of the F-tests for equality of variances. Note the p-values of 0 are simply rounded values, indicating that p was well below 0.01.",
        col.names = c("Behaviour", "Heavy Users", "Light Users", "Difference","t-value", "p-value"))
```

## Appendix five - clout and vulnerability values
```{r clout, echo = FALSE}
clout_and_vuln_stats %>% 
  mutate(cust = toproper(cust),
         brand = toproper(brand)) %>% 
  kable(caption = "Clout and vulnerabilty values per brand for heavy and light users. Note that negative cross-price elasticities were not included in these calculations as they are believed to erroneous values introduced via aggregation.",
        col.names = c("Consumer", "Brand", "Clout", "Vulnerability"))
```